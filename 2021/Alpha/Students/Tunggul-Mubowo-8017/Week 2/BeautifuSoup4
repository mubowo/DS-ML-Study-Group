DSC Unilag DS/ML 
Week 2 Assignment Report : BeautifulSoup4
Student : Tunggul Sutrambuko Mubowo

## DSC DS/ML Week 2 Assessment

Deadline: 9pm GMT+1 31-01-2020

Scraping Brief
Website:  [All products | Books to Scrape - Sandbox](http://books.toscrape.com/)
Detail: Books to Scrape is a site built for the sole purpose of scraping practice. It contains a list of 1000 books.

Task: Create a scraper that crawls through the website and scrapes details about all 1000 books. For each book, collect the:
Name
Image URL
Price
Rating

These details are to be stored in a pandas dataframe.
Enjoy.


# Import Libraries
import pandas as pd
from bs4 import BeautifulSoup as bs
from urllib.request import urlopen
import re

# define the url
url = "http://books.toscrape.com/"

# get the html from the page and assign it to the html variable
html = urlopen(url)

# parse the html and assign it to the variable soup
soup = bs(html.read(), 'html.parser')

# single item operation on get the text of the  name, image URL, price, rating
name = soup.find("h3").find("a").get("title")
image_url = soup.find("img", class_="thumbnail").get("src")
price = soup.find("p", class_="price_color").text
rating = soup.find("p", class_= "star-rating Three").get("class")[1]

print(name)
print(image_url)
print(price)
print(rating)

# converts to Pandas Dataframe
df1 = pd.DataFrame({
    "Name" : name,
    "Image URL" : image_url,
    "Price" : price,
    "Rating" : [rating]
})

# saving to csv file 
df1.to_csv("first_book_to_scrape.csv", index=False)

# preview the saved csv file
df1.head()

# single page operation on get the text of the  name, image URL, price, rating

names = []
image_urls = []
prices = []
ratings = []

for i in soup("li", class_="col-xs-6 col-sm-4 col-md-3 col-lg-3" ):
  name = i.find("h3").find("a").get("title")
  names.append(name)

  image_url = i.find("img", class_="thumbnail").get("src")
  image_urls.append(image_url)

  price = i.find("p", class_="price_color").text
  prices.append(price)

  rating = i.find("p", class_= re.compile("star-rating")).get("class")[1]
  ratings.append(rating)

# display the result
print(names)
print(image_urls)
print(prices)
print(ratings)


# convert single page operation (spo) into pandas dataframe

df2 = pd.DataFrame({
    "Name" : names,
    "Image URL" : image_urls,
    "Price" : prices,
    "Rating" : ratings
})

# saving single page operation (spo) into CSV file
df2.to_csv("first_book_to_scrape_spo.csv", index=False)

# preview the single page operation csv file
df2.head()

## Creating all 50 pages of books.toscrape.com web

# creating all url webpage
web_pages = []
for j in range(1,50):
  urls = "http://books.toscrape.com/catalogue/page-" + str(j) + ".html"
  web_pages.append(urls)

# change the data extraction code to a for loop that will process the URLs one after the other and store all the data in a variable4

# set all variable
whole_names = []
whole_image_urls = []
whole_prices = []
whole_ratings = []

# loop through the web_pages and find the  <li> list-item elements containing the name, image-url, price & rating
for k in web_pages:
  html = urlopen(url)
  soup = bs(html.read(), 'html.parser')
  contents = soup("li", class_="col-xs-6 col-sm-4 col-md-3 col-lg-3" )

  # loop through the content of each web_page 
  for l in contents:
    name = l.find("h3").find("a").get("title")
    whole_names.append(name)

    image_url = l.find("img", class_="thumbnail").get("src")
    whole_image_urls.append(image_url)

    price = l.find("p", class_="price_color").text
    whole_prices.append(price)

    rating = l.find("p", class_= re.compile("star-rating")).get("class")[1]
    whole_ratings.append(rating)


# saving web_pages scraping into Panda DataFrame
df_whole = pd.DataFrame({
    "Name" : whole_names,
    "Image URL" : whole_image_urls,
    "Price" : whole_prices,
    "Rating" : whole_ratings
})

df_whole.to_csv("whole_pages.csv", index=False)

df_whole.head()

df_whole

